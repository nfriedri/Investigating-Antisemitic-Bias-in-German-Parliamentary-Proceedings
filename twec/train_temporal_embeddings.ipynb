{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-''\n",
    "from twec.twec import TWEC\n",
    "from gensim.models.word2vec import Word2Vec, PathLineSentences\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim import utils\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "import itertools\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = Path('../obj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(filepath):\n",
    "    with open(str(data_folder / filepath), 'rb') as f:\n",
    "        corpus = pickle.load(f)\n",
    "        return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create slices for TWEC - slices for both training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed slices\n",
    "slice_1_spell = load_corpus('slice_1_lem_spell.pkl')\n",
    "slice_2_spell = load_corpus('slice_2_lem_spell.pkl')\n",
    "slice_3_spell = load_corpus('slice_3_lem_spell.pkl')\n",
    "slice_4_spell = load_corpus('slice_4_lem_spell.pkl')\n",
    "slice_5_spell = load_corpus('slice_5_lem_spell.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_1_spell = list(itertools.chain(*slice_1_spell))\n",
    "slice_2_spell = list(itertools.chain(*slice_2_spell))\n",
    "slice_3_spell = list(itertools.chain(*slice_3_spell))\n",
    "slice_4_spell = list(itertools.chain(*slice_4_spell))\n",
    "slice_5_spell = list(itertools.chain(*slice_5_spell))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save slices in LineSentence format\n",
    "utils.save_as_line_sentence(slice_1_spell, os.path.join(Path('.\\\\examples\\\\training\\\\slice_1.txt')))\n",
    "utils.save_as_line_sentence(slice_2_spell, os.path.join(Path('.\\\\examples\\\\training\\\\slice_2.txt')))\n",
    "utils.save_as_line_sentence(slice_3_spell, os.path.join(Path('.\\\\examples\\\\training\\\\slice_3.txt')))\n",
    "utils.save_as_line_sentence(slice_4_spell, os.path.join(Path('.\\\\examples\\\\training\\\\slice_4.txt')))\n",
    "utils.save_as_line_sentence(slice_5_spell, os.path.join(Path('.\\\\examples\\\\training\\\\slice_5.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_corpus = slice_1_spell + slice_2_spell + slice_3_spell + slice_4_spell + slice_5_spell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_as_line_sentence(full_corpus,'.\\\\examples\\\\training\\\\compass.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the compass.\n",
      "Compass will be overwritten after training\n",
      "New train_model\n"
     ]
    }
   ],
   "source": [
    "# standard params gensim.Word2Vec iter=5, ns=10 \n",
    "aligner = TWEC(size=200, siter=5, diter=5, workers=1, ns=10)\n",
    "aligner.train_compass(\"examples/training/compass\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'hs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-ae210393ee94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maligner\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTWEC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mditer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'.\\\\examples\\\\testing'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'hs'"
     ]
    }
   ],
   "source": [
    "# This param setting is needed to evaluate the temporal embedding spaces --> no negative samples and hierarchical softmax applied \n",
    "# aligner = TWEC(size=200, siter=10, diter=10, workers=4, ns=0, hs=1, test='.\\\\examples\\\\testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training temporal embeddings: slice examples/training/compass/slice_1.txt.\n",
      "New train_model\n",
      "Initializing temporal embeddings from the atemporal compass.\n",
      "Training temporal embeddings: slice examples/training/compass/slice_2.txt.\n",
      "New train_model\n",
      "Initializing temporal embeddings from the atemporal compass.\n",
      "Training temporal embeddings: slice examples/training/compass/slice_3.txt.\n",
      "New train_model\n",
      "Initializing temporal embeddings from the atemporal compass.\n",
      "Training temporal embeddings: slice examples/training/compass/slice_4.txt.\n",
      "New train_model\n",
      "Initializing temporal embeddings from the atemporal compass.\n",
      "Training temporal embeddings: slice examples/training/compass/slice_5.txt.\n",
      "New train_model\n",
      "Initializing temporal embeddings from the atemporal compass.\n"
     ]
    }
   ],
   "source": [
    "slice_1 = aligner.train_slice(\"examples/training/compass/slice_1.txt\", save=True)\n",
    "slice_2 = aligner.train_slice(\"examples/training/compass/slice_2.txt\", save=True)\n",
    "slice_3 = aligner.train_slice(\"examples/training/compass/slice_3.txt\", save=True)\n",
    "slice_4 = aligner.train_slice(\"examples/training/compass/slice_4.txt\", save=True)\n",
    "slice_5 = aligner.train_slice(\"examples/training/compass/slice_5.txt\", save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "compass = Word2Vec.load(\"model/compass.model\") \n",
    "model1 = Word2Vec.load(\"model/slice_1.model\")\n",
    "model2 = Word2Vec.load(\"model/slice_2.model\")\n",
    "model3 = Word2Vec.load(\"model/slice_3.model\")\n",
    "model4 = Word2Vec.load(\"model/slice_4.model\")\n",
    "model5 = Word2Vec.load(\"model/slice_5.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "compass = compass.wv\n",
    "model_1 = model1.wv\n",
    "model_2 = model2.wv\n",
    "model_3 = model3.wv\n",
    "model_4 = model4.wv\n",
    "model_5 = model5.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160889\n",
      "52598\n",
      "51592\n",
      "60557\n",
      "68109\n",
      "62447\n"
     ]
    }
   ],
   "source": [
    "print(len(compass.vocab))\n",
    "print(len(model_1.vocab))\n",
    "print(len(model_2.vocab))\n",
    "print(len(model_3.vocab))\n",
    "print(len(model_4.vocab))\n",
    "print(len(model_5.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "compass.save_word2vec_format('model/compass.txt', binary=True)\n",
    "model_1.save_word2vec_format('model/slice_1.txt', binary=True)\n",
    "model_2.save_word2vec_format('model/slice_2.txt', binary=True)\n",
    "model_3.save_word2vec_format('model/slice_3.txt', binary=True)\n",
    "model_4.save_word2vec_format('model/slice_4.txt', binary=True)\n",
    "model_5.save_word2vec_format('model/slice_5.txt', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create slices for TWEC - slices for both training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_and_test_sets(train, split=0.05):\n",
    "    test = []\n",
    "    for i in range(0,round(split * len(train))):\n",
    "        random_number = np.random.randint(0,len(train))\n",
    "        # Remove doc at given index and add to test set\n",
    "        test.append(train.pop(random_number))\n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_1_train, slice_1_test = generate_train_and_test_sets(slice_1_spell)\n",
    "slice_2_train, slice_2_test = generate_train_and_test_sets(slice_2_spell)\n",
    "slice_3_train, slice_3_test = generate_train_and_test_sets(slice_3_spell)\n",
    "slice_4_train, slice_4_test = generate_train_and_test_sets(slice_4_spell)\n",
    "slice_5_train, slice_5_test = generate_train_and_test_sets(slice_5_spell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save testing slices\n",
    "gensim.utils.save_as_line_sentence(slice_1_test, os.path.join(Path('.\\\\twec\\\\examples\\\\testing\\\\slice_1_test.txt')))\n",
    "gensim.utils.save_as_line_sentence(slice_2_test, os.path.join(Path('.\\\\twec\\\\examples\\\\testing\\\\slice_2_test.txt')))\n",
    "gensim.utils.save_as_line_sentence(slice_3_test, os.path.join(Path('.\\\\twec\\\\examples\\\\testing\\\\slice_3_test.txt')))\n",
    "gensim.utils.save_as_line_sentence(slice_4_test, os.path.join(Path('.\\\\twec\\\\examples\\\\testing\\\\slice_4_test.txt')))\n",
    "gensim.utils.save_as_line_sentence(slice_5_test, os.path.join(Path('.\\\\twec\\\\examples\\\\testing\\\\slice_5_test.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('feindschaft', 0.7777882218360901),\n",
       " ('antipathie', 0.7710212469100952),\n",
       " ('widerwillen', 0.7268883585929871),\n",
       " ('verachtung', 0.7198671698570251),\n",
       " ('feindseligkeit', 0.7176477909088135),\n",
       " ('abneigung', 0.6986110806465149),\n",
       " ('groll', 0.693888783454895),\n",
       " ('animosität', 0.6788550615310669),\n",
       " ('verbitterung', 0.6756240725517273),\n",
       " ('unbilden', 0.6729105710983276)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('feindschaft', 0.7855952382087708),\n",
       " ('mißgunst', 0.7646018266677856),\n",
       " ('feindseligkeit', 0.7410603761672974),\n",
       " ('gehässigkeit', 0.7298420667648315),\n",
       " ('neid', 0.7282276153564453),\n",
       " ('abneigung', 0.7243658304214478),\n",
       " ('mißtrauen', 0.6989201307296753),\n",
       " ('antipathie', 0.6822444200515747),\n",
       " ('wut', 0.6798548698425293),\n",
       " ('verbitterung', 0.6706445217132568)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('feindschaft', 0.7284114360809326),\n",
       " ('verachtung', 0.7158603072166443),\n",
       " ('neid', 0.705698549747467),\n",
       " ('groll', 0.6960954666137695),\n",
       " ('antipathie', 0.6945077776908875),\n",
       " ('mißtrauen', 0.6902238726615906),\n",
       " ('hetze', 0.6819382309913635),\n",
       " ('abneigung', 0.6748616695404053),\n",
       " ('verbitterung', 0.6705626249313354),\n",
       " ('wut', 0.6676822900772095)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('feindschaft', 0.7295164465904236),\n",
       " ('mißgunst', 0.7236109972000122),\n",
       " ('neid', 0.6853048801422119),\n",
       " ('groll', 0.672009289264679),\n",
       " ('abneigung', 0.6471599340438843),\n",
       " ('vorurteil', 0.6460909843444824),\n",
       " ('mißtrauen', 0.6455237865447998),\n",
       " ('hetze', 0.6418379545211792),\n",
       " ('wut', 0.6321764588356018),\n",
       " ('verachtung', 0.6308550238609314)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('wut', 0.7203575968742371),\n",
       " ('feindschaft', 0.7080402374267578),\n",
       " ('neid', 0.65813148021698),\n",
       " ('leidenschaft', 0.6579920053482056),\n",
       " ('vorurteil', 0.6500828266143799),\n",
       " ('verachtung', 0.6427229642868042),\n",
       " ('aufbäumen', 0.636103630065918),\n",
       " ('gift', 0.6285501718521118),\n",
       " ('hetze', 0.6277855634689331),\n",
       " ('abscheu', 0.6212161779403687)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.most_similar('haß')\n",
    "model_2.most_similar('haß')\n",
    "model_3.most_similar('haß')\n",
    "model_4.most_similar('haß')\n",
    "model_5.most_similar('haß')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jude:\n",
      "0.5276708\n",
      "0.5594604\n",
      "0.5391677\n",
      "0.5198425\n",
      "\n",
      "\n",
      "Christ:\n",
      "0.47117013\n",
      "0.41780522\n",
      "0.39033422\n",
      "0.45575985\n"
     ]
    }
   ],
   "source": [
    "jude_1 = model_1['jude']\n",
    "jude_2 = model_2['jude']\n",
    "jude_3 = model_3['jude']\n",
    "jude_4 = model_4['jude']\n",
    "jude_5 = model_5['jude']\n",
    "christ_1 = model_1['christ']\n",
    "christ_2 = model_2['christ']\n",
    "christ_3 = model_3['christ']\n",
    "christ_4 = model_4['christ']\n",
    "christ_5 = model_5['christ']\n",
    "\n",
    "print('Jude:')\n",
    "print(np.dot(jude_1, jude_2) / (np.linalg.norm(jude_1) * np.linalg.norm(jude_2)))\n",
    "print(np.dot(jude_1, jude_3) / (np.linalg.norm(jude_1) * np.linalg.norm(jude_3)))\n",
    "print(np.dot(jude_1, jude_4) / (np.linalg.norm(jude_1) * np.linalg.norm(jude_4)))\n",
    "print(np.dot(jude_1, jude_5) / (np.linalg.norm(jude_1) * np.linalg.norm(jude_5)))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Christ:')\n",
    "print(np.dot(jude_1, christ_2) / (np.linalg.norm(jude_1) * np.linalg.norm(christ_2)))\n",
    "print(np.dot(jude_1, christ_3) / (np.linalg.norm(jude_1) * np.linalg.norm(christ_3)))\n",
    "print(np.dot(jude_1, christ_4) / (np.linalg.norm(jude_1) * np.linalg.norm(christ_4)))\n",
    "print(np.dot(jude_1, christ_5) / (np.linalg.norm(jude_1) * np.linalg.norm(christ_5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (master)",
   "language": "python",
   "name": "master"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
